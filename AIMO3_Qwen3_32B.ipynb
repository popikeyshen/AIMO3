{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 118448,
          "databundleVersionId": 14559231,
          "sourceType": "competition"
        },
        {
          "sourceId": 248118764,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 276793,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 237027,
          "modelId": 258700
        },
        {
          "sourceId": 375840,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 310551,
          "modelId": 322000
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "AIMO3-Qwen3-32B",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Ex5rHcfi6Lu6"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ai_mathematical_olympiad_progress_prize_3_path = kagglehub.competition_download('ai-mathematical-olympiad-progress-prize-3')\n",
        "mccocoful_latest_mdc_whls_path = kagglehub.notebook_output_download('mccocoful/latest-mdc-whls')\n",
        "qwen_lm_qwq_32b_transformers_qwq_32b_awq_1_path = kagglehub.model_download('qwen-lm/qwq-32b/Transformers/qwq-32b-awq/1')\n",
        "qwen_lm_qwen_3_transformers_32b_awq_1_path = kagglehub.model_download('qwen-lm/qwen-3/Transformers/32b-awq/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "lyp-Dx4z6Lu9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! uv pip uninstall --system 'tensorflow'\n",
        "! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'vllm' 'triton' 'numpy<2'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:42:54.169003Z",
          "iopub.execute_input": "2025-12-02T18:42:54.169183Z",
          "iopub.status.idle": "2025-12-02T18:43:09.128931Z",
          "shell.execute_reply.started": "2025-12-02T18:42:54.169165Z",
          "shell.execute_reply": "2025-12-02T18:43:09.12842Z"
        },
        "id": "XpqioxEl6Lu-",
        "outputId": "31e92800-00e9-49a4-a31d-d5e38883f2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 3.29s\u001b[0m\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.18.0\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m155 packages\u001b[0m \u001b[2min 435ms\u001b[0m\u001b[0m                                       \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m52 packages\u001b[0m \u001b[2min 10.07s\u001b[0m\u001b[0m                                           \n\u001b[2mUninstalled \u001b[1m25 packages\u001b[0m \u001b[2min 359ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m52 packages\u001b[0m \u001b[2min 56ms\u001b[0m\u001b[0m                               \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250622\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.9.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.116.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.115.14\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.7\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.3.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.12.0.88\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.37.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.37.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.37.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.37.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.37.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.58b0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.47b0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions-ai\u001b[0m\u001b[2m==0.4.9\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==4.25.8\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.7\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.47.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.46.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.8.5.post1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.18\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "set_seed(2024)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:43:09.129992Z",
          "iopub.execute_input": "2025-12-02T18:43:09.130143Z",
          "iopub.status.idle": "2025-12-02T18:43:15.659655Z",
          "shell.execute_reply.started": "2025-12-02T18:43:09.130128Z",
          "shell.execute_reply": "2025-12-02T18:43:15.659166Z"
        },
        "id": "TCHYmdi16Lu-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "import torch\n",
        "import kaggle_evaluation.aimo_3_inference_server\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "cutoff_time = time.time() + (4 * 60 + 45) * 60"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:43:15.66025Z",
          "iopub.execute_input": "2025-12-02T18:43:15.660464Z",
          "iopub.status.idle": "2025-12-02T18:43:16.958669Z",
          "shell.execute_reply.started": "2025-12-02T18:43:15.660451Z",
          "shell.execute_reply": "2025-12-02T18:43:16.958179Z"
        },
        "id": "4Vn4rowC6Lu_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def clean_memory(deep=False):\n",
        "    gc.collect()\n",
        "    if deep:\n",
        "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "llm_model_pth = '/kaggle/input/qwen-3/transformers/32b-awq/1'\n",
        "\n",
        "llm = LLM(\n",
        "    llm_model_pth,\n",
        "    #dtype=\"half\",                -> Changed this\n",
        "    #max_num_seqs=128,            -> Changed this\n",
        "    max_model_len=32768,#4096*10,\n",
        "    trust_remote_code=True,\n",
        "    tensor_parallel_size=1,\n",
        "    gpu_memory_utilization=0.96,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:43:16.959357Z",
          "iopub.execute_input": "2025-12-02T18:43:16.959633Z",
          "iopub.status.idle": "2025-12-02T18:49:56.504149Z",
          "shell.execute_reply.started": "2025-12-02T18:43:16.959619Z",
          "shell.execute_reply": "2025-12-02T18:49:56.503678Z"
        },
        "id": "SDJJ1F6v6Lu_",
        "outputId": "6e045289-daa3-49e7-e45d-92c12b44921b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "INFO 12-02 18:43:37 [__init__.py:239] Automatically detected platform cuda.\nINFO 12-02 18:43:54 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\nINFO 12-02 18:43:55 [awq_marlin.py:113] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 12-02 18:43:55 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\nWARNING 12-02 18:43:56 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\nINFO 12-02 18:44:00 [__init__.py:239] Automatically detected platform cuda.\nINFO 12-02 18:44:04 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen-3/transformers/32b-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen-3/transformers/32b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen-3/transformers/32b-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 12-02 18:44:04 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7e2f698ff4d0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[W1202 18:44:05.413660097 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1202 18:44:05.414298338 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "INFO 12-02 18:44:05 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 12-02 18:44:05 [cuda.py:221] Using Flash Attention backend on V1 engine.\nWARNING 12-02 18:44:05 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nINFO 12-02 18:44:05 [gpu_model_runner.py:1329] Starting to load model /kaggle/input/qwen-3/transformers/32b-awq/1...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:42<02:06, 42.21s/it]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [01:13<01:11, 35.58s/it]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [01:40<00:31, 31.61s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [02:22<00:00, 36.04s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [02:22<00:00, 35.72s/it]\n\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "INFO 12-02 18:46:28 [loader.py:458] Loading weights took 142.95 seconds\nINFO 12-02 18:46:30 [gpu_model_runner.py:1347] Model loading took 18.1453 GiB and 144.989844 seconds\nINFO 12-02 18:46:52 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/922129f9e7/rank_0_0 for vLLM's torch.compile\nINFO 12-02 18:46:52 [backends.py:430] Dynamo bytecode transform time: 21.90 s\nINFO 12-02 18:46:57 [backends.py:136] Cache the graph of shape None for later use\nINFO 12-02 18:47:50 [backends.py:148] Compiling a graph for general shape takes 56.45 s\nINFO 12-02 18:49:11 [monitor.py:33] torch.compile takes 78.35 s in total\nINFO 12-02 18:49:13 [kv_cache_utils.py:634] GPU KV cache size: 211,376 tokens\nINFO 12-02 18:49:13 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 6.45x\nINFO 12-02 18:49:56 [gpu_model_runner.py:1686] Graph capturing finished in 43 secs, took 1.40 GiB\nINFO 12-02 18:49:56 [core.py:159] init engine (profile, create kv cache, warmup model) took 206.03 seconds\nINFO 12-02 18:49:56 [core_client.py:439] Core engine process 0 ready.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = llm.get_tokenizer()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.504689Z",
          "iopub.execute_input": "2025-12-02T18:49:56.504848Z",
          "iopub.status.idle": "2025-12-02T18:49:56.507366Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.504835Z",
          "shell.execute_reply": "2025-12-02T18:49:56.50694Z"
        },
        "id": "49ZADm5Q6Lu_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#prompts\n",
        "thoughts = [\n",
        "\n",
        "    'Please use chained reasoning to put the answer in \\\\boxed{}.',\n",
        "    'Please reflect and verify while reasoning and put the answer in \\\\boxed{}.',\n",
        "    'Solve the following problem using concise and clear reasoning by placing the answer in \\\\boxed{}.',\n",
        "    'You are a helpful and reflective maths assistant, please reason step by step to put the answer in \\\\boxed{}.',\n",
        "    'You are the smartest maths expert in the world, please spike this question and put the answer in \\\\boxed{}.'\n",
        "]\n",
        "\n",
        "#create single prompt\n",
        "def make_next_prompt(text,round_idx):\n",
        "    default_prompt = thoughts[(round_idx+1)%len(thoughts)]\n",
        "    default_python_code = f\"print('{default_prompt}')\"\n",
        "    return default_python_code\n",
        "\n",
        "#extract python code from response\n",
        "def extract_python_code(text):\n",
        "    pattern = r'```python\\s*(.*?)\\s*```'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    if matches:\n",
        "        ans = \"\\n\\n\".join(matches)\n",
        "        #print(f'Extracted python code: {ans}')\n",
        "        return ans\n",
        "    return \"\"\n",
        "\n",
        "#extract all code segments\n",
        "def extract_python_code_list(text):\n",
        "    pattern = r'```python\\s*(.*?)\\s*```'\n",
        "    ans=[]\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    for m in matches:\n",
        "        ans.append(m)\n",
        "    return ans\n",
        "\n",
        "#process the code\n",
        "def process_python_code(query):\n",
        "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
        "    current_rows = query.strip().split(\"\\n\")\n",
        "    new_rows = []\n",
        "    for row in current_rows:\n",
        "        new_rows.append(row)\n",
        "    ans = \"\\n\".join(new_rows)\n",
        "    print(f'Processed python code: {ans}')\n",
        "    return ans\n",
        "\n",
        "import re\n",
        "\n",
        "#extract the answer from the boxes\n",
        "def extract_boxed_texts(text):\n",
        "    pattern = r'oxed{(.*?)}'\n",
        "    matches = re.findall(pattern, text)\n",
        "    if not matches:\n",
        "        return []\n",
        "    ans = []\n",
        "    for content in matches:\n",
        "        if content.isdigit():\n",
        "            num = int(content)\n",
        "        else:\n",
        "            nums = re.findall(r'\\d+', content)\n",
        "            if not nums:\n",
        "                continue\n",
        "            num = int(nums[-1])\n",
        "        ans.append(num % 100000)\n",
        "    return ans\n",
        "\n",
        "#extract the integer answer modulo 100000 from the boxes\n",
        "def extract_boxed_text(text):\n",
        "    pattern = r'oxed{(.*?)}'\n",
        "    matches = re.findall(pattern, text)\n",
        "    if not matches:\n",
        "        return -1\n",
        "    content = matches[0]\n",
        "    if content.isdigit():\n",
        "        num = int(content)\n",
        "    else:\n",
        "        nums = re.findall(r'\\d+', content)\n",
        "        if not nums:\n",
        "            return -1\n",
        "        num = int(nums[-1])\n",
        "    return num % 100000\n",
        "\n",
        "#select the final answer based on the frequency/ majoity voting\n",
        "from collections import Counter\n",
        "def select_answer(answers):\n",
        "    valid_answers = []\n",
        "    for answer in answers:\n",
        "        try:\n",
        "            if int(answer) == float(answer):\n",
        "                valid_answers.append(int(answer)%100000)\n",
        "        except:\n",
        "            pass\n",
        "    if not valid_answers:\n",
        "        return 49\n",
        "    _, answer = sorted([(v,k) for k,v in Counter(valid_answers).items()], reverse=True)[0]\n",
        "    return answer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.50786Z",
          "iopub.execute_input": "2025-12-02T18:49:56.508006Z",
          "iopub.status.idle": "2025-12-02T18:49:56.522323Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.507994Z",
          "shell.execute_reply": "2025-12-02T18:49:56.521923Z"
        },
        "id": "dfdoVZyz6Lu_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "import subprocess\n",
        "\n",
        "#Python REPL to execute code. taken from NuminaMath Solution\n",
        "class PythonREPL:\n",
        "    def __init__(self, timeout=8):\n",
        "        self.timeout = timeout\n",
        "\n",
        "    def __call__(self, query):\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
        "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(query)\n",
        "\n",
        "            try:\n",
        "                result = subprocess.run(\n",
        "                    [\"python3\", temp_file_path],\n",
        "                    capture_output=True,\n",
        "                    check=False,\n",
        "                    text=True,\n",
        "                    timeout=self.timeout,\n",
        "                )\n",
        "            except subprocess.TimeoutExpired:\n",
        "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
        "\n",
        "            stdout = result.stdout.strip()\n",
        "            stderr = result.stderr.strip()\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                return True, stdout\n",
        "            else:\n",
        "                # Process the error message to remove the temporary file path\n",
        "                # This makes the error message cleaner and more user-friendly\n",
        "                error_lines = stderr.split(\"\\n\")\n",
        "                cleaned_errors = []\n",
        "                for line in error_lines:\n",
        "                    if temp_file_path in line:\n",
        "                        # Remove the path from the error line\n",
        "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
        "                    cleaned_errors.append(line)\n",
        "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
        "                # Include stdout in the error case\n",
        "                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n",
        "                return False, combined_output\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.523666Z",
          "iopub.execute_input": "2025-12-02T18:49:56.523837Z",
          "iopub.status.idle": "2025-12-02T18:49:56.537286Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.523826Z",
          "shell.execute_reply": "2025-12-02T18:49:56.536864Z"
        },
        "id": "T3zVdES-6LvA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_texts = [\n",
        "    tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    for messages in [[{\"role\": \"user\", \"content\": \"hi\"}]]\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.537751Z",
          "iopub.execute_input": "2025-12-02T18:49:56.537892Z",
          "iopub.status.idle": "2025-12-02T18:49:56.572457Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.537878Z",
          "shell.execute_reply": "2025-12-02T18:49:56.571791Z"
        },
        "id": "S5o6-__r6LvA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#define the sampling parameters\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.95,              # Controls randomness in generation: higher values (e.g., 1.0) produce more diverse output.\n",
        "    min_p=0.01,                   # Minimum cumulative probability for nucleus sampling, filtering out unlikely tokens.\n",
        "    skip_special_tokens=True,\n",
        "    # max_tokens=1800,\n",
        "    max_tokens=32768,             # Sets a very high limit for token generation to handle longer outputs.\n",
        "    # stop=[\"```output\"],\n",
        ")\n",
        "\n",
        "#generate prompts in batch\n",
        "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
        "    list_of_texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            conversation=messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        for messages in list_of_messages\n",
        "    ]\n",
        "\n",
        "    request_output = llm.generate(\n",
        "        prompts=list_of_texts,\n",
        "        sampling_params=sampling_params,\n",
        "    )\n",
        "\n",
        "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
        "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
        "        print(messages[-1])\n",
        "\n",
        "    return list_of_messages\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.572933Z",
          "iopub.execute_input": "2025-12-02T18:49:56.573059Z",
          "iopub.status.idle": "2025-12-02T18:49:56.57688Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.573048Z",
          "shell.execute_reply": "2025-12-02T18:49:56.576466Z"
        },
        "id": "cTssMECX6LvB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_message_filter(list_of_messages,list_of_idx) -> tuple[list[list[dict]], list[str]]:\n",
        "    global answer_contributions\n",
        "    extracted_answers = []\n",
        "    list_of_messages_to_keep = []\n",
        "    list_of_idx_to_keep = []\n",
        "    for idx,messages in zip(list_of_idx,list_of_messages):\n",
        "        answers = extract_boxed_texts(messages[-1]['content'])\n",
        "        if answers:\n",
        "            extracted_answers.extend(answers)\n",
        "            for answer in answers:\n",
        "                answer_contributions[answer].append(idx)\n",
        "        else:\n",
        "            list_of_messages_to_keep.append(messages)\n",
        "            list_of_idx_to_keep.append(idx)\n",
        "    return list_of_messages_to_keep, extracted_answers, list_of_idx_to_keep"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.577342Z",
          "iopub.execute_input": "2025-12-02T18:49:56.577475Z",
          "iopub.status.idle": "2025-12-02T18:49:56.591834Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.577465Z",
          "shell.execute_reply": "2025-12-02T18:49:56.591401Z"
        },
        "id": "hw7KygMx6LvB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#execute the codes in the responses\n",
        "def batch_message_execute(list_of_messages,round_idx) -> list[list[dict]]:\n",
        "    for messages in list_of_messages:\n",
        "        python_code = extract_python_code(messages[-1]['content'],round_idx)\n",
        "        python_code = process_python_code(python_code)\n",
        "        try:\n",
        "            success, output = PythonREPL()(python_code)\n",
        "        except Exception as e:\n",
        "            output = str(e)\n",
        "        messages.append({'role': 'user', 'content': output})\n",
        "        print(messages[-1])\n",
        "    return list_of_messages\n",
        "\n",
        "#execute the code and generate the answer from responses\n",
        "def batch_message_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n",
        "    ans = []\n",
        "    for messages in list_of_messages:\n",
        "        python_code = extract_python_code(messages[-1]['content'])\n",
        "        python_code = process_python_code(python_code)\n",
        "        try:\n",
        "            success, output = PythonREPL()(python_code)\n",
        "            if success:\n",
        "                patten = r'(\\d+)'\n",
        "                matches = re.findall(patten, output)\n",
        "                if matches:\n",
        "                    for match in matches:\n",
        "                        ans.append(int(match)%100000)\n",
        "                        ans.append(int(match)%100000) #代码权重高于自然语言，所以添加两次\n",
        "        except Exception as e:\n",
        "            output = str(e)\n",
        "        print(f'python code output: {output}')\n",
        "    return ans\n",
        "\n",
        "#execute code and generate answer for all elements in batch\n",
        "def batch_message_list_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n",
        "    ans = []\n",
        "    for messages in list_of_messages:\n",
        "        python_code_list = extract_python_code_list(messages[-1]['content'])\n",
        "        for python_code in python_code_list:\n",
        "            python_code = process_python_code(python_code)\n",
        "            try:\n",
        "                success, output = PythonREPL()(python_code)\n",
        "                if success:\n",
        "                    patten = r'(\\d+)'\n",
        "                    matches = re.findall(patten, output)\n",
        "                    if matches:\n",
        "                        for match in matches:\n",
        "                            ans.append(int(match)%100000)\n",
        "                            ans.append(int(match)%100000)\n",
        "            except Exception as e:\n",
        "                output = str(e)\n",
        "            print(f'python code output: {output}')\n",
        "    return ans\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.592304Z",
          "iopub.execute_input": "2025-12-02T18:49:56.59243Z",
          "iopub.status.idle": "2025-12-02T18:49:56.603031Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.592421Z",
          "shell.execute_reply": "2025-12-02T18:49:56.602611Z"
        },
        "id": "pYL-c4Zx6LvB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "#API for competition submission\n",
        "import kaggle_evaluation.aimo_3_inference_server"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.603477Z",
          "iopub.execute_input": "2025-12-02T18:49:56.603619Z",
          "iopub.status.idle": "2025-12-02T18:49:56.615422Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.603609Z",
          "shell.execute_reply": "2025-12-02T18:49:56.615035Z"
        },
        "id": "-9pJWirn6LvC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#correct answers for reference problems\n",
        "def get_correct_answer(question):\n",
        "    if 'Let $ABC$ be an acute-angled triangle with integer side' in question: return 336\n",
        "    if 'Define a function $f \\colon \\mathbb{Z}_{\\geq 1} \\to \\mathbb{Z}_{\\geq 1}$' in question: return 32951\n",
        "    if 'A tournament is held with $2^{20}$ runners each of which' in question: return 21818\n",
        "    if 'On a blackboard, Ken starts off by' in question: return 32193\n",
        "    if 'Let $ABC$ be a triangle with $AB \\neq AC$, circumcircle' in question: return 57447\n",
        "    if 'Let $n \\geq 6$ be a positive integer' in question: return 8687\n",
        "    if 'Alice and Bob are each holding some integer number of sweets' in question: return 50\n",
        "    if 'Let $f \\colon \\mathbb{Z}_{\\geq 1} \\to \\mathbb{Z}_{\\geq 1}$ be a function' in question: return 580\n",
        "    if 'A $500 \\times 500$ square is divided into $k$ rectangles' in question: return 520\n",
        "    if 'Let $\\mathcal{F}$ be the set of functions $\\alpha \\colon \\mathbb{Z}\\to' in question: return 160\n",
        "    return 0"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.61586Z",
          "iopub.execute_input": "2025-12-02T18:49:56.615987Z",
          "iopub.status.idle": "2025-12-02T18:49:56.625016Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.615975Z",
          "shell.execute_reply": "2025-12-02T18:49:56.624647Z"
        },
        "id": "EArtJP5D6LvC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "g_score = 0\n",
        "g_count = 0\n",
        "prompt_score = Counter()\n",
        "answer_contributions = defaultdict(list)\n",
        "def predict_for_question(question: str) -> int:\n",
        "    global g_score\n",
        "    global g_count\n",
        "    global prompt_score\n",
        "    global answer_contributions\n",
        "    question += \"\\nIf the final answer is a number larger than 100000, take modulo 100000. \"\n",
        "    if time.time() > cutoff_time:\n",
        "        return 210\n",
        "    print(question)\n",
        "\n",
        "    list_of_messages = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": thoughts[k]},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ] for k in range(5)\n",
        "    ]\n",
        "\n",
        "    all_extracted_answers = []\n",
        "    list_of_idx = list(range(len(list_of_messages)))\n",
        "    max_round = 1\n",
        "    for round_idx in range(max_round):\n",
        "        print(f\"round {round_idx+1}\")\n",
        "        list_of_messages = batch_message_generate(list_of_messages)\n",
        "        #extracted_python_answer = batch_message_execute_and_get_answer(list_of_messages,round_idx)\n",
        "        extracted_python_answer = batch_message_list_execute_and_get_answer(list_of_messages,round_idx)\n",
        "        list_of_messages, extracted_answers, list_of_idx  = batch_message_filter(list_of_messages, list_of_idx)\n",
        "        all_extracted_answers.extend(extracted_python_answer)\n",
        "        all_extracted_answers.extend(extracted_answers)\n",
        "        print(\"extracted boxed answers:\",extracted_answers)\n",
        "        print(\"extracted python answers:\",extracted_python_answer)\n",
        "        print(\"all extracted answers:\",all_extracted_answers)\n",
        "        if not list_of_messages:\n",
        "            break\n",
        "        #list_of_messages = batch_message_execute(list_of_messages,round_idx)\n",
        "    answer = select_answer(all_extracted_answers)\n",
        "    print(\"answer:\",answer)\n",
        "    correct_answer = get_correct_answer(question)\n",
        "    print(\"correct answer:\",correct_answer)\n",
        "    g_count += 1\n",
        "    if str(answer) == str(correct_answer):\n",
        "        g_score += 1\n",
        "\n",
        "    print(f\"score: {g_score}/{g_count}\")\n",
        "    print(\"\\n\\n\")\n",
        "    return answer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.625421Z",
          "iopub.execute_input": "2025-12-02T18:49:56.62554Z",
          "iopub.status.idle": "2025-12-02T18:49:56.635109Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.625529Z",
          "shell.execute_reply": "2025-12-02T18:49:56.634726Z"
        },
        "id": "c52fMG5I6LvC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
        "    id_ = id_.item(0)\n",
        "    print(\"------\")\n",
        "    print(id_)\n",
        "\n",
        "    question = question.item(0)\n",
        "    answer = predict_for_question(question)\n",
        "    print(question)\n",
        "    print(\"------\\n\\n\\n\")\n",
        "    return pl.DataFrame({'id': id_, 'answer': answer})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.63553Z",
          "iopub.execute_input": "2025-12-02T18:49:56.635655Z",
          "iopub.status.idle": "2025-12-02T18:49:56.649205Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.635645Z",
          "shell.execute_reply": "2025-12-02T18:49:56.648845Z"
        },
        "id": "sevekuVW6LvC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\n",
        "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv'\n",
        ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.649621Z",
          "iopub.execute_input": "2025-12-02T18:49:56.649742Z",
          "iopub.status.idle": "2025-12-02T18:49:56.721458Z",
          "shell.execute_reply.started": "2025-12-02T18:49:56.649734Z",
          "shell.execute_reply": "2025-12-02T18:49:56.721062Z"
        },
        "id": "67mzjvfs6LvC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway(\n",
        "        (\n",
        "            'reference.csv',\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T18:49:56.721943Z",
          "iopub.execute_input": "2025-12-02T18:49:56.722077Z"
        },
        "id": "2CFmB9CO6LvC",
        "outputId": "ccccf36f-73d1-42ae-de4b-2f12312919d1",
        "colab": {
          "referenced_widgets": [
            "169ef12f57294869b1134e080339d1d1"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "------\n86e8e5\nLet $n \\geq 6$ be a positive integer. We call a positive integer $n$-Norwegian if it has three distinct positive divisors whose sum is equal to $n$. Let $f(n)$ denote the smallest $n$-Norwegian positive integer. Let $M=3^{2025!}$ and for a non-negative integer $c$ define \n\\begin{equation*}\n    g(c)=\\frac{1}{2025!}\\left\\lfloor \\frac{2025! f(M+c)}{M}\\right\\rfloor.\n\\end{equation*}\nWe can write \n\\begin{equation*}\n    g(0)+g(4M)+g(1848374)+g(10162574)+g(265710644)+g(44636594)=\\frac{p}{q}\n\\end{equation*}\nwhere $p$ and $q$ are coprime positive integers. What is the remainder when $p+q$ is divided by $99991$?\nIf the final answer is a number larger than 100000, take modulo 100000. \nround 1\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "169ef12f57294869b1134e080339d1d1"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df = pd.read_parquet('submission.parquet').sort_values('id')\n",
        "    print(df)\n",
        "except Exception:\n",
        "    print('No file generated!')"
      ],
      "metadata": {
        "trusted": true,
        "id": "gNVmc7IJ6LvC"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}